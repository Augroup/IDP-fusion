#!/usr/bin/env python
import sys, re, os
import sam_basics, sequence_basics, psl_basics, genepred_basics

# Pre:   <artificial fusion info file> - First id may be unique to each proposed long read fusion
#                                        Next ID is the first name of the artificial fusion construct
#                                          chr13_5000_6000_+/chr22_2000_3000_-
#                                          where the coordintes "zero-based, half open like genepred and psl formats
#                                          direction sign indicates whether the reference sequence (+) or its reverse complement (-) was put into the fasta
#                                        next two numbers ... i wonder, only the first two entries will be used to help annotate outputs for downstream processing
#                                        Caution: Right now we depend on this file ordering the fasta header labels in the same order the reference sequences appear
#        <uniquely named short reads file> - can be generated by ./make_uniquely_named_short_read_file.py
#                                            can be a fastq or a fasta, and each name must be different
#        <split read aligner sam file> - Starting with the output of the star aligner
#                                               Reference reads are in a name ref_001 
#                                               where the integer at the end is the same as its order in the <artifical fusion fasta>
#        <reference read count file> - for each read the number of times it maps in the reference genome and or transcriptome
#        <psl file of fusion candidates> - Must be the output of "change_psl_4digit_fusion.py or 
#                                          "change_psl_polyA3end4digit_fusion.py" in the case of known primers
#                                          probably named "newname4_LR.bestpsl_fusion_pair"
#        <uniqueness bedgraph file> - bedGraph format file <chromosome> <start> <end> <mappability (0 to 1, 1 is uniquely mappable)>  
#                                     bedGraph has 0-indexed start and 1-indexed end coordinates. 
#        <junctions output file> - where to write the output junctions
#        <fusion genepred output file> - where to write the output fusion genepred
#        <temp folder name> - name of a folder we can work in
#        <min search distance> - an integer.  search for short read support of long read junction will extend AT LEAST this far
#        <max gap/overlap> - an integer.  search for short read support of long read jucntion will expand no farther than this
#        <L_min_intron> - an integer.  this is the minimimum gap we consider an intron.  other blocks in long read psl files will be chuncked together

# Modifies:  Populates the  temp folder above with files necessary for 
#        counting read contributions fusion events
#        All outputs here that have corrinates are output in one-index
#        1.) temp/uniqueness/fusion_read.map
#            This file is for showing how each read maps to the artifical fusion fastas
#            tab delimited:
#            <read name> <artifical fusion index> <mapping location> <junction location>
#        2.) temp/uniqueness/artificial_fusion_names.txt
#            This file is for converting the index of an artifical fusion to its name used in its sam file
#            <artifical fusion index> <artifical fusion name>
#        3.) temp/uniqueness/long_read_locations.txt
#            This file has the junction evidence observed in the long reads 
#            <artifical fusion index> <fusion id> <fasta header> <left junction> <right junction>
#        4.) temp/uniqueness/fusion_read_counts.txt
#            This file has the number of distinct locations spanning artifical fusions that each read maps to
#            This will give us uniq and multimap counts for each read
#            <read name> <count>

#  post: Writes out a "junctions.txt" type file to the filename you select, 
#        a tab delimited file with entries as follows
#    <fusion id> - derived from a piece of long read evidence
#    <artifical fusion name> - used for split read mapping short reads
#    <fasta header> - another name used for the artifical fusion name
#                     of the format chromosome, 0-indexed base, 1-indexed base, direction of sequence in artifical fusion, forward slash then the same
#    <left junction> - if junction is on the left side of the read the value indicates
#                        the start, or leftmost position of the read before the breakpoit
#                        and is 0-indexed
#                      if junction is on the right side of the read the value indicated is
#                        the end, or rightmost base coordinate of the read before the breakpoint
#                        and is 1-indexed
#    <right junction> - if junction is on the left side of the read the value indicates
#                        the start, or leftmost position of the read before the breakpoit
#                        and is 0-indexed
#                      if junction is on the right side of the read the value indicated is
#                        the end, or rightmost base coordinate of the read before the breakpoint
#                        and is 1-indexed
#   <Type 1 nonredundant> - Number of distinct yet uniquely mapped positions that support this junction
#                           (unique meaning a read maps to one place alone in the fusion, and nothing in reference)
#                           (distinct positions means that multiple reads mapping to the exact same position will be counted once)
#   <Type 1 unique> - Number of uniquely mapped reads that support his junction
#   <Type 2> - Number of reads that are multi-mapped to multiple fusion sites but is never observed in the reference
#   <Type 3> - Number of reads that are uniquely mapped to a reference position
#   <Type 4> - Number of reads that are multi-mapped to reference positions
#   <long read direction> - direction the long read evidence was oriented with respect to the artifical fusion
#                           this may be useful if putting together some polished reporting
#                           its the last field in the LR_fusion_candidates.fa.info file.
#   <junction count> - Number of different junctions within our long read fusion point margin of error
#   <junction string> - a string that starts with the index-1 base thats the last base before and the last base after the
#                       the long read fusion breakpoint.  Then it is followed by a "|" then 
#                       junctions (short read) and their read support counts for each junction 
#                       within our margine or long read fusion point error.  
#                       These coordiantes are 1-indexed base positions for the coordiante just before the breakpoint on either side.


gap_or_overlap_search_multiplier = 2.5
# search for long read and short read where distance between the two
# is less than min search distance
# or is less than the larger of either gap or overlap value times 
#    the above variable, but is less than the max search distance

def main():
  if len(sys.argv) != 13:
    print sys.argv[0] + ' <artificial fusion info filename> <uniquely named short reads file> <split read aligner sam file> <reference read count> <psl file of fusion candidates> <uniqueness bedGraph> <junction output file> <fusion genepred out file> <temp directory> <min junction search distance> <max gap/overlap size> <L_min_intron>'
    sys.exit()

  artificial_fusion_info_filename = sys.argv[1]
  sruniq_filename = sys.argv[2]
  split_sam_filename = sys.argv[3]
  reference_read_count_filename = sys.argv[4]
  psl_filename = sys.argv[5]
  uniqueness_bedgraph_filename = sys.argv[6]
  # can be '-' to indicate no data
  output_file = sys.argv[7]
  output_genepred_filename = sys.argv[8]
  temp_foldername = sys.argv[9]
  min_search_distance = int(sys.argv[10])
  max_gap_overlap = int(sys.argv[11])
  min_gap_in_block = int(sys.argv[12]) # this value is used for smoothing indels when doing the psl to genepred conversion so there not so many blocks
                       # L_min_intron is used for this
  if not os.path.isdir(temp_foldername):  
    print "Error:  Expecting a temporary folder that already exists."
    print temp_foldername + " does not exist."
    sys.exit()

  local_temp_foldername = temp_foldername.rstrip('/')+'/uniqueness'

  if not os.path.isdir(local_temp_foldername):
    print "Creating subdirectory "+local_temp_foldername
    os.system("mkdir "+local_temp_foldername)

  #1. parse the artifical fusion info file
  print "parse the artificial fusion info file"
  conv = parse_fusion_info_file(artificial_fusion_info_filename)

  #2. save converted long-read junction information
  long_read_locations = local_temp_foldername.rstrip('/')+'/long_read_locations.txt'
  print "save the long read junction coordinates"
  write_long_read_locations(conv,long_read_locations)

  #3. Use the psl file to get gap and overlap information for each FID
  print "get gap and overlap information for each FID from the psl file"
  gap_overlap_per_fid = get_gap_or_overlap_from_psl_file(psl_filename)

  #4. go through the sam file converting coordiantes
  print "go through the sam file converting coordinates."
  fusion_read_map = local_temp_foldername.rstrip('/')+'/fusion_read.map'
  artifical_fusion_names = revert_coordinates_in_split_sam(split_sam_filename,conv,fusion_read_map)

  #5. gather how unqiue the mapping of each read spanning a fusion is
  fusion_read_counts = local_temp_foldername.rstrip('/') + '/fusion_read_counts.txt'
  print "save the individual fusion read counts and bring them into a data structure"
  # just use a not so pretty system call to group each unique mapping/read and then group again on read only to count reads mapping to multiple places
  os.system("cut -f 1,3 "+fusion_read_map+" | sort | uniq | cut -f 1 | sort | uniq -c | sed 's/^[ ]*//' | sed 's/[ ]/\t/' | awk '{print $2,$1}' | sed 's/[ ]/\t/' > " + fusion_read_counts)
  fusion_read_counts = get_read_counts(fusion_read_counts)

  #6. get the counts of reads from reference
  reference_read_counts = get_reference_read_counts(reference_read_count_filename,fusion_read_counts)

  #7. for every read classify it as 
  #   1. unique to a fusion location, not seen in the reference
  #   2. multiply mappable to fusions, not seen in the reference
  #   3. uniquely mappable to the reference
  #   4. multiply mappable to the reference
  print "assign read type classifications"
  read_classifications = get_read_classifications(reference_read_counts, fusion_read_counts)

  #8. Go through the fusion read map file and judge what kind of evidence each
  #   read lends to each fusion event
  print "sort out all read evidence"
  [fusion_evidence, fusion_info] = read_evidence(fusion_read_map,conv,read_classifications,artifical_fusion_names,gap_overlap_per_fid,min_search_distance,max_gap_overlap)

  #9. build a junctions.txt file output
  print "build the first part of the junctions.txt file"
  [fid_lines, fid_coords] = build_junctions_txt_file(fusion_evidence,fusion_info)

  #10. For fusion ID's we have evidence for, convert the psl file into a genepred file
  #    we need our accurate junction point information so we can make observations about the base unqiueness
  print "make the fusion genepred file"
  fusion_gpds = make_fusion_genepred(psl_filename,fid_coords,min_gap_in_block,output_genepred_filename)

  #11. For fusion ID's we have evidence for get uniqueness information around the short read junction point
  print "set all fusion coordiantes on any fusion long read to zero"
  fusion_coordinates = initialize_fusion_coordinates(fusion_gpds)
  print "get the uniqueness value for all fusion coordinates"
  # modifies fusion_coordinates
  get_coordinate_uniqueness(uniqueness_bedgraph_filename, fusion_coordinates)
  print "calculate the uniqueness of each fusion"
  fid_least_unique = get_fusion_uniqueness(fusion_gpds, fusion_coordinates)  

  #12. Now we can enrich our per-long read file with information about the
  #    shortest long read support around around the junction
  print "add annotations to junctions.txt file"
  add_annotations_per_fusion(fid_lines,fid_least_unique, output_file)


def add_annotations_per_fusion(fid_lines, fid_least_unique, output_file):
  ofile = open(output_file,'w')
  for fid in fid_lines:
    ofile.write(fid_lines[fid]+"\t")
    ofile.write(str(fid_least_unique[fid]['shortest_total_length'])+"\t")
    ofile.write(str(fid_least_unique[fid]['shortest_unique_count'])+"\t")
    osets = []
    proxdic = fid_least_unique[fid]['lowest_proximal_unique_counts']
    proxdists = map(int,proxdic.keys())
    proxdists.sort()
    for dist in proxdists:
      osets.append(str(dist)+':'+str(proxdic[dist]))
    ostring = ",".join(osets)
    ofile.write(ostring+"\n")
  ofile.close()

# pre: genepred entries dictionary - keyed by fid with a list of two entries per fid
#      fusion_coordinates - for every fusion coordinate it holds the uniqueness of zero or 1
# post: for each fusion, check the left and right sides in the list, and get the total length of the long read support, and also get the uniqueness in the sequences near by the fusion events.
#       this will be the shortest value or lowest unique count for either side of the fusion
#       we get: shortest total length, lowest total unique base count, and lowest flanking unique count for various distances proximal to the fusion point
def get_fusion_uniqueness(fusion_gpds, fusion_coordinates):
  output = {}
  for fid in fusion_gpds:
    #get uniqueness of proximal 10, 50, 100
    proxcheck = [10, 50, 100] # these are the sizes to check
    proxuniqcount = {}
    shortest_length = 9999999999
    shortest_unique = 9999999999
    for line in fusion_gpds[fid]:
      [g, side] = line
      total_base_count = 0
      unique_base_count = 0
      unique_array = []
      for i in range(0,g['exonCount']):
        for j in range(g['exonStarts'][i],g['exonEnds'][i]):
          total_base_count += 1
          if g['chrom'] in fusion_coordinates:
            coord = j+1
            if coord in fusion_coordinates[g['chrom']]:
              unique_array.append(fusion_coordinates[g['chrom']][coord])
              if fusion_coordinates[g['chrom']][coord] == 1:
                unique_base_count+=1
            else:
              unique_array.append(0)
          else:
            unique_array.append(0)
      #now for this line we have our unique counts
      # save the shortest
      if total_base_count < shortest_length: shortest_length = total_base_count
      if unique_base_count < shortest_unique: shortest_unique = unique_base_count
      if side == '-': #fusion is on the right side
        unique_array.reverse() # reverse in place so the first elements are proximal to the junction
      for dist in proxcheck:
        sum = 0
        maxdist = dist
        if maxdist > len(unique_array): maxdist = len(unique_array)
        for k in range(0,maxdist): 
          sum += unique_array[k]
        if dist not in proxuniqcount:
          proxuniqcount[dist] = int(sum)
        elif sum < proxuniqcount[dist]:
          proxuniqcount[dist] = int(sum)  # set it to the lower value
    #Now we have our shortest results to report back
    o = {}
    o['shortest_total_length'] = shortest_length
    o['shortest_unique_count'] = shortest_unique
    o['lowest_proximal_unique_counts'] = proxuniqcount
    output[fid] = o  
  return output    

#pre: uniqueness_bedgraph is a bed graph format file with unique entries labled 1
#     fusion_coordiantes is initialized to all values equal to zero it is a dictionary keyed by chromosome then 1 indexed coordinate
#post: returns nothing, but it modifies fusion_coordinates to have the uniqueness score
#
def get_coordinate_uniqueness(uniqueness_bedgraph_filename, fusion_coordinates):
  if uniqueness_bedgraph_filename=='-': return
  with open(uniqueness_bedgraph_filename) as infile:
    for line in infile:
      f = line.rstrip().split("\t")
      chrom = f[0]
      start = int(f[1])
      finish = int(f[2])
      uniqueness = float(f[3])
      if uniqueness != 1: continue  # we are only interested in cataloging unique entries right now
      for i in range(start, finish):
        if chrom in fusion_coordinates:
          coord = i+1
          if coord in fusion_coordinates[chrom]:
            fusion_coordinates[chrom][coord] = uniqueness
  return

# pre: the per fusion dictionary of genepreds 
# post: a dictionary keyed by chromosome then 1-indexed coordinate initalized to zero
def initialize_fusion_coordinates(fusion_gpds):
  d = {}
  for fid in fusion_gpds:
    for line in fusion_gpds[fid]:
      entry = line[0]
      if entry['chrom'] not in d:
        d[entry['chrom']] = {}  
      for i in range(0,entry['exonCount']):
        for j in range(entry['exonStarts'][i],entry['exonEnds'][i]):
          d[entry['chrom']][j+1] = 0
  return d

# We are going to make an attempt at making LR_fusion.gpd
# We will try to keep the name thats being used the same except for
# changing the length and the fusion point. 
# It looks like there is some smooothing done to ignore indels
# where blocks are combined.
# pre: a <psl file>
#      query name is of the format:
#        A_B|C[+/-]D|ccs[+/-]E
#        A: identity
#        B: length
#        C: F# for fusion id
#        +/-: Indicates whether the fusion occurs at the start of the alignment or end
#        D: pos where the junction occurs
#           NOTE:  This is what we really need to know what this format actually is so we can recode position correctly.  We know where the true fusion point is.  
#                  And in this function we pass in the 1-indexed base of the coordinate of the last base of the fragment proximal to the fusion event
#        +/-: optional
#        E: optional
#      <fid_coords> is a data structure with left and right coordinates
#        these have already been switche according to the long read direction
#        found in the long read info file to be on the correct side.
#        coordinates are index base 1.  
#      <min gap in block size>  this i used to smooth out the genepred file a bit so blocks are more like exons
#        gaps smaller than this get joined together.
#      <output fusion genepred filename>
# post: write a genepred file
#       the gene_name is taken from qName in the psl file.
#       the name is updated to have the length of genepred entry, and
#       the new fusion position.
#       save the genepred entries in a dictionary keyed by fusion
#       that has first and second entries in an array, and each of those entries has a genepred entry and a +(left) -(right) side the fusion is on
def make_fusion_genepred(psl_filename,fid_coords,min_gap_in_block_size,outfile):
  i = 0
  ofile = open(outfile,'w')
  fusion_gpd = {}
  with open(psl_filename) as infile:
    for line in infile:
      m = i % 2
      i += 1
      entry = psl_basics.read_psl_entry(line)
      gline = psl_basics.convert_entry_to_genepred_line(entry)
      d = genepred_basics.genepred_line_to_dictionary(gline)
      # m == 0 is left, m == 1 is right
      look = re.search('\|(F[\d]+)([+-])',d['gene_name'])
      fid = look.group(1)
      fsidesign = look.group(2) #which side is the fusion on
      if fid not in fid_coords: continue # we only are working on the ones that met criteria for short reads

      #fix our end points based on short read defined fusions
      side = 'left'
      if m == 1: side = 'right'
      [chr, coord, dir] = fid_coords[fid][side]

      coord = int(coord)
      # fix our endpoints
      e = {} # our corrected genepred
      newpos = 0 #this will be used to update the 
      if fsidesign == '-':
        #print 'right ' +str(coord) + " " + str(d['exonEnds'][d['exonCount']-1])
        # if the new txEnd is greater than the last exon, we need to move it out
        if coord == d['exonEnds'][d['exonCount']-1]:
          e = d
          #print "right side stays the same"
        elif coord > d['exonEnds'][d['exonCount']-1]:
          e = genepred_basics.right_extend_genepred(d,coord)
          #print "right extend transcript"
          #everything is all set
        else:
          e = genepred_basics.right_trim_genepred(d,coord)
          #print "right shorten transcript"
        newpos = e['txEnd']
      else:
        # end point is on the left side of the alignment
        #print 'left ' + str(coord) + " " + str(d['exonStarts'][0])
        if coord-1 == d['exonStarts'][0]:
          e = d
          #print "left stays the same"
        if coord-1 < d['exonStarts'][0]:
          e = genepred_basics.left_extend_genepred(d,coord-1)
          #print "left extend transcript"
        else:
          e = genepred_basics.left_trim_genepred(d,coord-1)
          #print "left trim transcript"
        newpos = e['txStart']
      # we treat cdsStart and cdsEnd the same as tx
      e['cdsStart'] = e['txStart']        
      e['cdsEnd'] = e['txEnd']        
      
      f = genepred_basics.smooth_gaps(e,min_gap_in_block_size)
 
      # get new transcript length
      newlen = 0
      for j in range(0,f['exonCount']): newlen += f['exonEnds'][j] - f['exonStarts'][j]

      # make a new name based on the new fusion site and new length
      look = re.match('^([\d\.]+_)\d+(\|F\d+[+-])\d+(\|.*)$',f['gene_name'])
      v1 = look.group(1)
      v2 = str(newlen)
      v3 = look.group(2)
      v4 = str(newpos)
      v5 = look.group(3)
      f['gene_name'] = v1 + v2 + v3 + v4 + v5 

      gpline = genepred_basics.genepred_entry_to_genepred_line(f)
      ofile.write(gpline+"\n")
      if fid not in fusion_gpd: 
        n = [0,0]
      fusion_gpd[fid] = n
      fusion_gpd[fid][m] = [f, fsidesign] #send back the genepred and the side the fusion is on
  ofile.close()
  return fusion_gpd

# pre: fusion_evidence is a data structure keyed by fusion id from our long read evidence
#                      each entry is a dictonary keyed by junctions that are supporting it
#                        meaning those short reads occur within the max_search_distance of the long read evidence
#                        the key itself contains the chromosome and one-indexed base position of the last base before the breakpoint
#                      each entry contains an array of 5 sets.  Entries in these sets are deterimined of reads type 1-4.
#                        index 0: type 1 mappings, a set of non redundant unique reads supporting the junction
#                        index 1: type 1 reads, a set of reads mapping to only this junction coordiante and nothing in the reference
#                        index 2: type 2 reads, a set of reads mapping to multiple fusion events but nothing in the reference
#                        index 3: type 3 reads, a set of reads mapping mapping to one position in the reference
#                        index 4: type 4 reads, a set of reads mapping to multiple locations in the reference
#      fusion_info is a dictionary keyed by fusion id that has rname, the name of the artifical
#                  fusion.  It also contains fasta_header, the alternate name for the fusion that actually describes how the construct is put together
#      junction_outfile is a filename where to write the new junctions.txt
#  post: Builds the first part of a "junctions.txt" type file, a tab delimited file with entries as follows
#    <fusion id> - derived from a piece of long read evidence
#    <artifical fusion name> - used for split read mapping short reads
#    <fasta header> - another name used for the artifical fusion name
#                     of the format chromosome, 0-indexed base, 1-indexed base, direction of sequence in artifical fusion, forward slash then the same
#    <left junction> - if junction is on the left side of the read the value indicates
#                        the start, or leftmost position of the read before the breakpoit
#                        and is 0-indexed
#                      if junction is on the right side of the read the value indicated is
#                        the end, or rightmost base coordinate of the read before the breakpoint
#                        and is 1-indexed
#    <right junction> - if junction is on the left side of the read the value indicates
#                        the start, or leftmost position of the read before the breakpoit
#                        and is 0-indexed
#                      if junction is on the right side of the read the value indicated is
#                        the end, or rightmost base coordinate of the read before the breakpoint
#                        and is 1-indexed
#   <Type 1 nonredundant> - Number of distinct yet uniquely mapped positions that support this junction
#                           (unique meaning a read maps to one place alone in the fusion, and nothing in reference)
#                           (distinct positions means that multiple reads mapping to the exact same position will be counted once)
#   <Type 1 unique> - Number of uniquely mapped reads that support his junction
#   <Type 2> - Number of reads that are multi-mapped to multiple fusion sites but is never observed in the reference
#   <Type 3> - Number of reads that are uniquely mapped to a reference position
#   <Type 4> - Number of reads that are multi-mapped to reference positions
#   <long read direction> - direction the long read evidence was oriented with respect to the artifical fusion
#                           this may be useful if putting together some polished reporting
#                           its the last field in the LR_fusion_candidates.fa.info file.
#   <junction count> - Number of different junctions within our long read fusion point margin of error
#   <junction string> - a string that starts with the index-1 base thats the last base before and the last base after the
#                       the long read fusion breakpoint.  Then it is followed by a "|" then 
#                       junctions (short read) and their read support counts for each junction 
#                       within our margin then a distance between long read and short read.  
#                       These coordiantes are 1-indexed base positions for the coordiante just before the breakpoint on either side.
# This is all saved to a dictionary keyed by fusion ID
# we also save a left and right for the 1-based fusion coordiante (best coordinate).
# the selection of that point is important for downstream functions to another dictionary
def build_junctions_txt_file(fusion_evidence,fusion_info):
  output = {}
  outcoords = {}
  for fid in fusion_evidence:
    #find the junction with the best evidence = most unique read support
    bestcount = 0
    bestjunction = ''
    junction_count = len(fusion_evidence[fid])
    evidence_strings = []
    # Choose a best junction to describe this based on one that has the most unique reads supporting it (type 1).  if the unique read count is zero, then best multimappedfusion read support (type 2)
    for junction in fusion_evidence[fid]:
      enr = len(fusion_evidence[fid][junction][0])
      e1 = len(fusion_evidence[fid][junction][1])
      e2 = len(fusion_evidence[fid][junction][2])
      e3 = len(fusion_evidence[fid][junction][3])
      e4 = len(fusion_evidence[fid][junction][4])
      dist = fusion_evidence[fid][junction][5]
      estring = junction + '[' + str(enr) + ',' + str(e1) + ',' + str(e2) + ',' + str(e3) + ',' + str(e4) +']'+str(dist)
      evidence_strings.append(estring)
      if e1 > bestcount: 
        bestcount = e1
        bestjunction = junction
    if bestcount == 0: # no unique evidence try multi
      for junction in fusion_evidence[fid]:
        e2 = len(fusion_evidence[fid][junction][2])
        if e2 > bestcount:
          bestcount = e2
          bestjunction = junction
    if bestjunction == '': continue
    enr = len(fusion_evidence[fid][bestjunction][0])
    e1 = len(fusion_evidence[fid][bestjunction][1])
    e2 = len(fusion_evidence[fid][bestjunction][2])
    e3 = len(fusion_evidence[fid][bestjunction][3])
    e4 = len(fusion_evidence[fid][bestjunction][4])
    rname = fusion_info[fid]['rname']
    longdirection = fusion_info[fid]['breaks']['direction']
    longbreak = fusion_info[fid]['breaks']['left_true'] + '/' + fusion_info[fid]['breaks']['right_true']
    fasta_header = fusion_info[fid]['fasta_header']
    [j1,j2] = bestjunction.split('/')
    [j1chrom, j1coord] = j1.split(":")
    [j2chrom, j2coord] = j2.split(":")
    m = re.search('_([+-])/.+_([+-])$',fasta_header)
    if not m: 
      print "fasta header not formated as expected."
      sys.exit()
    leftdir = m.group(1)
    rightdir = m.group(2)
    struct = {}  # we need to use this data again for getting uniqueness calculations
    struct['left'] = [j1chrom, int(j1coord), leftdir] #save our 1-based coordiantes for use down the line before we start playing with them for junctions.txt file format
    struct['right'] = [j2chrom, int(j2coord), rightdir]
    struct['direction'] = longdirection
    if longdirection == '-':  # actually need to switch it to go back to the original genepred and do the correct left and right
      struct['right'] = [j1chrom, int(j1coord), opposite(leftdir)] #save our 1-based coordiantes for use down the line before we start playing with them for junctions.txt file format
      struct['left'] = [j2chrom, int(j2coord), opposite(rightdir)]
    # prepare outputs for the format of junctions.txt
    # if the junction is on the left side of the read its integer 
    #   indicates the start, or leftmost, position of the short read
    #   this is zero based
    # if the junction is on the right side of the read the integer
    #   indicates the end, or rightmost, position of the short read 
    #   this is one based
    if leftdir == '-':
      # junction is on the left side so we are required to make it zero based
      j1coord = str(int(j1coord)-1)
    if rightdir == '+':
      #junction is on the left side so we are required to make it zero based
      j2coord = str(int(j2coord)-1)
    evidence_string = ";".join(evidence_strings)
    evidence_string = longbreak + '|' + evidence_string
    output[fid] = fid + "\t" + rname + "\t" + fasta_header + "\t" + j1coord + "\t" + j2coord + "\t" + str(enr) + "\t" + str(e1) + "\t" + str(e2) + "\t" + str(e3) + "\t" + str(e4) + "\t" + longdirection + "\t" + str(junction_count) + "\t" + evidence_string
    outcoords[fid] = struct
  return [output, outcoords]

# pre: a psl file name
#      this psl must be the output of change_psl_4digit_fusion.py
#      or change_psl_polyA3end4digit_fusion.py
# post: a dictionary keyed by fusion ID (ie F0001) that has the
#       gap and overlap information in a dictionary
def get_gap_or_overlap_from_psl_file(psl_filename):
  res = {}
  with open(psl_filename) as infile:
    while True:
      line1 = infile.readline().strip()
      if not line1: return res
      line2 = infile.readline().strip()
      if not line2: return res
      e1 = psl_basics.read_psl_entry(line1)
      e2 = psl_basics.read_psl_entry(line2)
      prog = re.compile('\|(F\d+)[+-]')
      m = prog.search(e1['qName'])
      if not m: 
        print "problem reading query name from psl file"
        sys.exit()
      e1name = m.group(1)
      m = prog.search(e2['qName'])
      if not m: 
        print "problem reading query name from psl file"
        sys.exit()
      e2name = m.group(1)
      if e1name != e2name:
        print "problem: consecutive lines should have the same fusion ID"
        sys.exit()
      v = {}
      v['base_overlap'] = psl_basics.query_coordinates_base_overlap_size(e1,e2)
      v['region_overlap'] = psl_basics.query_coordinates_region_overlap_size(e1,e2)
      v['gap'] = psl_basics.query_coordinates_gap_size(e1,e2)
      res[e1name] = v
  return res

# pre: requires the fusion_read_map that has -
#         <read name> <artificial fusion index> <mapping> <junction>
#      conv - data structure which for each artificial index has each fusion and each long read coordinate
#      read_classification - which has the the type of each read (1-4)
#      artifical_fusion_names - a simple data structure for converting from artifical fusion index to the id used in junctions.txt
#      gap_overlap_per_fid - 
#      min_search_distance - AT LEAST how far away short read breakpoints can be from long read break points
#      max_gap_overlap - AT MOST how far away short read breakpoints can be from long read break points is max_gap_overlap*2.5
# post: an events data structure that is a dictionary keyed by fusion id
#         each fusion id contains a dictionary of splice junctions
#         each splice junction contains an array of read information
#         indexes 1 -4 correspond to sets of reads contributing to that classification of information
#         index 0 contains a set of nonredundant reads that uniquely map to that junction
#         index 5 contains the distance
#       an info data structure contains the artifical fusion name, and the fasta header for the artifical fusion for each fusion id
def read_evidence(fusion_read_map,conv,read_classification,artifical_fusion_names,gap_overlap_per_fid,min_search_distance,max_gap_overlap):
  events = {}
  info = {}
  with open(fusion_read_map) as mapfile:
    for line in mapfile:
      line = line.rstrip()
      [read, rid, mapping, junction] = line.split("\t")
      rid = int(rid)
      [j1,j2] = junction.split('/')
      [j1chrom, j1coord] = j1.split(":")
      [j2chrom, j2coord] = j2.split(":")
      j1coord = int(j1coord)
      j2coord = int(j2coord)
      rname = artifical_fusion_names[rid]
      coords = conv[rid]['fusions']
      #print mapping + " " + conv[rid]['fasta_header']
      for fid in coords:
        # get the largest distance between our short and long read evidence
        dist = maximum_distance(j1,j2,coords[fid]['left_true'],coords[fid]['right_true'])
        # get the larger of the gap or overlap size
        gapoverlapsize = max(gap_overlap_per_fid[fid]['base_overlap'],gap_overlap_per_fid[fid]['gap'])
        # get the recommended search range
        localmax = gap_or_overlap_search_multiplier*gapoverlapsize
        if localmax < min_search_distance: localmax = min_search_distance #search AT LEAST the minimum
        # recall that -1 means different chromosomes
        ### Warning.  We may prefer to use max_gap_overlap*gap_or_overlap_search_multiplier as the middle condition
        ### Warning.  There is a problem where a coordinate can be selected that is out of the bounds of the genepred.
        ### If we run into datasets in the future that throw this error we may need to further limit potential coordinates based on coordinatse that fall within the bounds of the genepred entry
        ### This may involve importing the genepred entry. try including the next line if you want to get that error
        #if dist == -1 or dist > max_gap_overlap*gap_or_overlap_search_multiplier or dist > localmax: continue
        if dist == -1 or dist > max_gap_overlap or dist > localmax: continue
        # still here then the coordinates support the long read junction
        if fid not in events:
          junctions = {}
          events[fid] = {}
          info[fid] = {}
          info[fid]['rname'] = rname
          info[fid]['fasta_header'] = conv[rid]['fasta_header']
          info[fid]['breaks'] = conv[rid]['fusions'][fid]
        if junction not in events[fid]:
          val1m = set()  #mappings
          val1 = set()   #read names for rest
          val2 = set()
          val3 = set()
          val4 = set()
          # for each hit type, keep track of the mappings
          events[fid][junction] = [val1m,val1,val2,val3,val4,dist]
        class_num = read_classification[read]
        # take advantage of type 1-4 in that array, but remember 0 is nonredudnat, and 5 is distance
        events[fid][junction][class_num].add(read)
        if(class_num == 1):
          events[fid][junction][0].add(mapping) #non redundant count based on mapping
  return [events, info]

# pre: two junction coordiantes for the short read, two junction coordinates for the long read
#      index 1 of the format chrX:10000
# post: If there are any ends not on the expected chromosome, return -1
#       retun the maxmimum distance  

def maximum_distance(s1,s2,l1,l2):
  [s1chrom, s1coord] = s1.split(":")
  [s2chrom, s2coord] = s2.split(":")
  s1coord = int(s1coord)
  s2coord = int(s2coord)
  [l1chrom, l1coord] = l1.split(":")
  [l2chrom, l2coord] = l2.split(":")
  l1coord = int(l1coord)
  l2coord = int(l2coord)
  if s1chrom != l1chrom or s2chrom != l2chrom:
    return -1
  dist = 0 
  abs1 = abs(s1coord - l1coord)
  abs2 = abs(s2coord - l2coord)
  if abs1 > abs2:
    return abs1
  return abs2
  

#   pre: read count dictionaries for the fusion and reference
#   post: return a dictionary with each fusion read name classified as follows
#   1. unique to a fusion location, not seen in the reference
#   2. multiply mappable to fusions, not seen in the reference
#   3. uniquely mappable to the reference
#   4. multiply mappable to the reference
def get_read_classifications(reference_read_counts, fusion_read_counts):
  types = {}
  for name in fusion_read_counts:
    inreference = 1
    types[name] = 0
    if not name in reference_read_counts:
      #shouldn't get here because they all should be there
      inreference = 0
    elif reference_read_counts[name] == 0:
      inreference = 0

    if fusion_read_counts == 0:
      print "strange to have an uncounted one.  impossible. exit."
      sys.exit()
    elif fusion_read_counts[name] == 1 and inreference == 0:
      types[name] = 1 #type 1 is the best. unique evidence
    elif fusion_read_counts[name] > 1 and inreference ==0:
      types[name] = 2 #multiply mappable but only to fusions
    elif inreference == 0:
      print "impossible we should have delt with this situation. exit."
      sys.exit()
    elif inreference == 1 and reference_read_counts[name] == 1:
      types[name] = 3 #supports fusion but uniquely mappable to the reference
    elif inreference == 1 and reference_read_counts[name] > 1:
      types[name] = 4 #supports fusion but multiply mappable to the reference
    else:
      print "we should have covered all cases."
      sys.exit()
    if types[name] ==0:
      print "unable to asign type"
      sys.exit()
  return types

# pre: a file containing the <read name> <read count> for the reference
#      a dictionary of read names and counts for fusions
# post: a dictionary of read names and read counts of reference reads that
#       are part of the fusion set
def get_reference_read_counts(reference_read_count_filename,fusion_read_counts):
  reference_read_counts = {}
  with open(reference_read_count_filename) as rcfile:
    for line in rcfile:
      line = line.rstrip()
      [rname, rcount] = line.split("\t")
      if rname in fusion_read_counts:
        reference_read_counts[rname] = int(rcount)
  return reference_read_counts

# pre: file containing <read name> <read count>
# post: return dictionary containing <read name> <read count>
def get_read_counts(fusion_read_counts):
  readcount = {}
  with open(fusion_read_counts) as rcfile:
    for line in rcfile:
      line = line.rstrip()
      [rname, rcount] = line.split("\t")
      readcount[rname] = int(rcount)
  return readcount


# pre: conversion data structure from parse_fusion_info_file
# post: writes the <artifical fusion index> <long read fusion ID> <fasta header> <long read left junction> <long read right junction>
#       coordiantes are one based
#       the coordaintes are ordered according to the artifical fusion
# modifies: file IO
def write_long_read_locations(conv,ofilename):
  ofile = open(ofilename,'w')
  for i in conv:
    for fusion in conv[i]['fusions']:
      ofile.write(str(i) + "\t" + fusion + "\t" + conv[i]['fasta_header'] + "\t" + conv[i]['fusions'][fusion]['left_true'] + "\t" + conv[i]['fusions'][fusion]['right_true']+"\n")
  ofile.close()


# pre: split_sam_filename, conversion guide
# post: Write to a file, out_fusion_map, list of reads supporting fusion events
#       <readname> <arf index> <mapping> <junction>
#       <readname> is self explanitory
#       <arf index> is the index of the artifical reference sequence
#                   this can be used to access the data structure with all our
#                   original long-read-detected fusion points
#       <mapping> this is the one-based coordinates of where the read maps to
#          with the sign indicating the direction on the chromosome the mapping runs
#       <junction> the one base coordinate of the bases just before the fusion 
#          (the last base in the read before the fusion, and then the first base in the read after the fusion)
#    Return dicionary keyed on <artifical fusion index> set to <artifical fusion name>
#           this is not seemingly that important, but it is part of the junctions.txt file that is made
def revert_coordinates_in_split_sam(split_sam_filename,conv,out_fusion_map):
  ofile = open(out_fusion_map,'w')
  seennames = {}
  with open(split_sam_filename) as samfile:
    for line in samfile:
      line = line.rstrip()
      if re.match('^@[A-Z][A-Z]\s',line): continue #skip header
      d = sam_basics.sam_line_to_dictionary(line)
      if re.match('^\d+M$',d['cigar']): continue #skip obvious non-split reads
      m = re.match('^ref_(\d+)$',d['rname'])
      if not m:
        print "error:  can't understand sam file.  ref sequence not named according to idp-fusion convention ref_0001"
        sys.exit()
      rind = int(m.group(1))
      if rind not in seennames:
        seennames[rind]=d['rname']
      if rind not in conv: continue # WARNING WE SHOULD NOT NEED TO DO THIS
      # this function is very similar to sam_basics.convert_directionless_gpd_alignment_to_reference
      startposition = d['pos']-1
      if startposition > conv[rind]['conversion']['breakpoint']: continue
      readcoord = []
      leftreadcoord = []
      rightreadcoord = []
      leftreadbase = []
      rightreadbase = []
      z = 0 #count of position that will include the gaps used as an offset to find the coordinate in the reference
      y = 0 #count of position within the read only
      leftmost = -1
      leftchrom = ''
      rightmost  = -1
      rightchrom = ''
      #Use this later if we we want to pull out the read sequences
      #oriented_seq = d['seq']
      #if sam_basics.check_flag(d['flag'],0x10): 
      #  oriented_seq = sequence_basics.rc(d['seq'])
 
      # Do coordinate conversions using the cigar string and conv variable
      for entry in d['cigar_array']:
        if re.match('[MISX=]',entry['op']): # all entries that map to read
          for i in range(0,entry['val']):
            if re.match('[M=X]',entry['op']): #all entries that match the reference alginment
              coord = conv[rind]['conversion']['coordinates'][startposition+z]
              readcoord.append(conv[rind]['conversion']['coordinates'][startposition+z])
              [chrom, point] = coord.split(":")
              if startposition+z < conv[rind]['conversion']['breakpoint']:
                leftreadcoord.append(int(point))
                leftchrom = chrom
              else:
                rightreadcoord.append(int(point))
                rightchrom = chrom
              if leftmost == -1: leftmost = startposition+z
              if startposition+z > rightmost: rightmost = startposition+z
              z+=1
            # Use this later to deal with the bases of the read
            #if startposition+z < conv[rind]['conversion']['breakpoint']:
            #  leftreadbase.append(oriented_seq[y])
            #else:
            #  rightreadbase.append(oriented_seq[y])
            #y+=1        
        if re.match('[DNH]',entry['op']):
          z+= entry['val']

      # Focus on reads spanning the breakpoint 
      # the breakpoint saved in the 'conv' is the index 
      if leftmost < conv[rind]['conversion']['breakpoint'] and rightmost >= conv[rind]['conversion']['breakpoint']:
        # Use this later to deal with read bases and put together our read fragments
        #leftread_seq = "".join(leftreadbase)
        #rightread_seq = "".join(rightreadbase)
        #print leftread_seq + " " + rightread_seq

        # we may have coordinates in reverse order but we want nice abbreviations
        leftdir = '+'
        if leftreadcoord[0] > leftreadcoord[len(leftreadcoord)-1]:
          leftdir = '-'
          leftreadcoord.reverse()
        rightdir = '+'
        if rightreadcoord[0] > rightreadcoord[len(rightreadcoord)-1]:
          rightdir = '-'
          rightreadcoord.reverse()

        leftcollapse =  sequence_basics.collapse_coordinate_array(leftreadcoord)
        rightcollapse = sequence_basics.collapse_coordinate_array(rightreadcoord)

        # use direction information to get the true junction site
        leftreal = leftchrom+":"+str(leftreadcoord[len(leftreadcoord)-1])
        if leftdir == '-':
          leftreal = leftchrom+":"+str(leftreadcoord[0])
        rightreal = rightchrom+":"+str(rightreadcoord[len(rightreadcoord)-1])
        if rightdir == '+':
          rightreal = rightchrom+":"+str(rightreadcoord[0])

        mappingabbrev = leftchrom+":"+leftcollapse+leftdir+'/'+rightchrom+":"+rightcollapse+rightdir
        junctionabbrev = leftreal+"/"+rightreal

        # we will deal with assigning fusions later using what we read in from the info file
        # the output format here is
        #<Read name> <artificial reference index (for accessing datastructure)> <mapping abbreviation(for the mapping of the complete read)> <junction abbrev>
        ofile.write(d['qname'] + "\t" + str(rind) + "\t" + mappingabbrev + "\t" + junctionabbrev+"\n")
  ofile.close()
  return seennames

# pre: A header_coordinates file where each entry is keyed by its integer position (index 1) in the fasta file
#      Each entry contains left and right sides and each side has chromosome, start, end and direction information
# post: A conversion scheme keyed by the integer position of the artificial fusion fasta position
#       Each entry in the dictionary has an array where each point contains the chromosome and coordinate
#       example: dictionary entry keyed on 1 'coordinates' = [chr1:100,chr1:101,chr1:103,chr2:300,chr2:301]
#                                                            coordinates are one indexed
#                                            'breakpoint' = integer (zero based index of the first base of the right side of the fusion)
def get_conversion_scheme_header_coordinate(coord):
  l = coord['left']
  r = coord['right']
  llen = l['end']-l['start']
  rlen = r['end']-r['start']
  locs = []
  breakpoint = 0
  #do the left side
  if l['dir'] == '+':
    for i in range(l['start'],l['end']):
      locs.append(l['chr']+":"+str(i+1))
      breakpoint += 1
  else:
    for i in range(l['end']-1,l['start']-1,-1):
       locs.append(l['chr']+":"+str(i+1))
       breakpoint += 1
  # do the right side
  if r['dir'] == '+':
    for i in range(r['start'],r['end']):
      locs.append(r['chr']+":"+str(i+1))
  else:
    for i in range(r['end']-1,r['start']-1,-1):
      locs.append(r['chr']+":"+str(i+1))
  d = {}
  d['breakpoint'] = breakpoint
  d['coordinates'] = locs
  d['orientations'] = l['dir']+"/"+r['dir']
  return d

# pre: an info file with headers of the format created in IDP-fusion for
#      LR_fusion_candidate.fa where >chr1_1000_2000_+/chr2_3000_4000_-
#      sign indicates whether the original sequence (+) or its reverse complement (-) was put in the fasta
#      the left coordinate is the zero indexed first base of the pseudo-intronic region
#      the right coordinate is the one indexed last base of the pseudo-intronic region
# post: conv
#       a dictionary indexed by the integer position (one based) of the entries
#       position in the fasta file.  The dictionary contains a data structure with 
#       with conversions from each entry of the fasta file to its one-indexed coordiante in the genome
#       these are stored in an array under [ref_index]['conversion']
#       There is also a dictionary with artifical and true observed left and right break points
#       stored in [ref_index]['fusions'][fusion ID] that is keyed by fusion ID (F0001)
#       NEED TO FILL IN THE FORMAT OF THOSE COORDINATES
#       As well as the true left and true right breakpoints
def parse_fusion_info_file(infofilename):
  convs = {}
  seen = set() #keep track of what headers we've seen to give them the proper index number
  i = 0
  with open(infofilename) as infofile:
    for line in infofile:
      f = line.strip().split("\t")
      if int(f[2]) < 0 or int(f[3]) < 0: continue # A terrible way to get around a bug where there strange coordinates outside of the artifical reference size (negative and too big)
      if f[1] not in seen: # first time seeing this
        i+=1
        seen.add(f[1])
        m = re.match('^([^_]+)_([^_]+)_([^_]+)_([^_])\/([^_]+)_([^_]+)_([^_]+)_([^_])',f[1])
        if not m: sys.exit() # problem
        # only calculate conversion if this is the first time seeing this
        d = {}
        left = {}
        right = {}
        left['chr'] = m.group(1)
        left['start'] = int(m.group(2))
        left['end'] = int(m.group(3))
        left['dir'] = m.group(4)
        right['chr'] = m.group(5)
        right['start'] = int(m.group(6))
        right['end'] = int(m.group(7))
        right['dir'] = m.group(8)
        d['left'] = left
        d['right'] = right
        d['fasta_header'] = f[1]
        dconv = get_conversion_scheme_header_coordinate(d)
        temp = {}
        convs[i] = temp
        convs[i]['conversion'] = dconv
      #use our conversion to annotate the fusion
      dconv = convs[i]['conversion']
      leftartifical = int(f[2])-1 # This is the point in the artifical fasta where the leftmost
                  # base before the breakpoint is located.  This is zero based
      rightartifical = int(f[3]) # This is the point in the artfical fasta where the rightmost
                   # breakpoint is located.  This is zero based
                   # rightmost base before the break point
      lefttrue = dconv['coordinates'][leftartifical] 
      righttrue = dconv['coordinates'][rightartifical]
      breaks = {}
      breaks['left_artificial'] =  leftartifical
      breaks['right_artifical'] = rightartifical
      breaks['left_true'] = lefttrue
      breaks['right_true'] = righttrue
      breaks['direction'] = f[4]
      if not 'fusions' in convs[i]:
        temp2 = {}
        convs[i]['fusions'] = temp2
      convs[i]['fusions'][f[0]] = breaks
      convs[i]['fasta_header'] = f[1]
  return convs

def opposite(sign):
  if sign == '+':
    return '-'
  elif sign == '-':
    return '+'
  print "error strange sign"
  sys.exit()
  return 0
  
main()
